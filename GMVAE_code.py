# -*- coding: utf-8 -*-
"""VAE_model2.ipynb

Automatically generated by Colaboratory.
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch import randn
from torch.nn import MSELoss
from torch.optim import Adam
from torch.utils.data import DataLoader
import numpy as np
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import random
import pandas as pd
import seaborn as sns
import time
import pickle
from datetime import datetime
!pip install hickle
import hickle as hkl
from torch.autograd import Variable
import gzip
import sys
import os
import torch.optim as optim
from sklearn.decomposition import TruncatedSVD
torch.set_default_tensor_type(torch.DoubleTensor)

#https://github.com/deeptools/pyBigWig
!pip install pyBigWig
import pyBigWig

"""# Data Preparation"""

def chip_tensor(ulist):
  u1array = np.array(ulist).T
  u1array[u1array == None] = 0
  torchtransform = transforms.Compose([transforms.ToTensor()])
  u2 = torchtransform(np.array(u1array,dtype="float64"))
  # u2 = u2.view(6,1,100,100).transpose(1,0) #every row corresponds to 1 bin in HiC, 100 rows for 100 bins in HiC
  return u2

class JointDataset():
    def __init__(self, data, mode='train'):
        self.mode = mode
        self.data = data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, index):
        img = self.data[index]
        return img

# ['Dnase','H3k27ac','H3k4me3','H3k27me3','Ctcf','H3k36me3',"H3k04me1","H3k9me3"]
train_data = []
for i in [1,2,3,4,6]:
  print(i,datetime.now())
  chr1_list = hkl.load("/content/drive/MyDrive/Courses/STCS6701/Final/data/Dec2_chr"+str(i)+"_2000samples_res50bp_10bins_8tracks.hkl")
  chr1_array = [chip_tensor(chr1_list[i]) for i in range(len(chr1_list))]
  train_data.append(chr1_array)
train_data2 = [j for i in train_data for j in i]
all_train = JointDataset(train_data2)

"""# GMVAE"""

class InferenceNet(nn.Module):
  def __init__(self,x_dim=80,n_states=5,z_parameter = 8,gumbel_temperature=1.0):
    super(InferenceNet,self).__init__()
    self.x_dim = x_dim
    self.n_states = n_states
    self.latent_dim = z_parameter
    self.gumbel_temperature = gumbel_temperature
    self.FC_y = nn.Sequential(
        nn.Linear(self.x_dim,int(self.x_dim/2)),
        nn.ReLU(),
        nn.Linear(int(self.x_dim/2),int(self.x_dim/2)),
        nn.ReLU(),
        nn.Linear(int(self.x_dim/2),int(self.n_states))
    )
    self.FC_z = nn.Sequential(
        nn.Linear(int(self.x_dim+self.n_states),int(self.x_dim/2)),
        # nn.Linear(int(self.x_dim),int(self.x_dim/2)),
        nn.ReLU(),
        nn.Linear(int(self.x_dim/2),int(self.x_dim/2)),
        nn.ReLU()
    )
    self.z_mu_layers = nn.ModuleList()
    self.z_var_layers = nn.ModuleList()
    for s in range(self.n_states):
      self.z_mu_layers.append(nn.Linear(int(self.x_dim/2),int(self.latent_dim)))
      self.z_var_layers.append(nn.Linear(int(self.x_dim/2),int(self.latent_dim)))
  def gumbel_softmax(self,y1,temperature):
    y1a = F.softmax(y1,-1)
    U = torch.rand(y1a.shape[1])
    gumbel_noise = -torch.log(-torch.log(U+1e-10)+1e-10)
    updated_y1 = F.softmax((y1a + gumbel_noise)/temperature,-1)
    return updated_y1
  def gaussian_sample(self,mu,var):
    epsilon = torch.randn_like(torch.sqrt(var+1e-10))
    z_infer = mu + torch.sqrt(var+1e-10) * epsilon
    return z_infer
  def forward(self,x_input):
    x_input = torch.log(x_input+1)
    #1. q(y|x)
    y_vec = self.FC_y(x_input)     # n_state-length vector directly comes from FC
    # y_vec = y_vec + F.one_hot(torch.topk(y_vec,2).indices[:,1],5).type(torch.DoubleTensor)*0.1 #----------------!!!!
    y_prob = F.softmax(y_vec,-1) # softmax probability of the FC vector
    y_gumbel = self.gumbel_softmax(y_vec,self.gumbel_temperature) # gumbel_softmax for downstream prediction of z|x,y
    #2. q(z|x,y)
    z_vec = self.FC_z(torch.cat((x_input,y_gumbel),1))
    # z_vec = self.FC_z(x_input)
    idx = torch.argmax(y_gumbel)
    # print(idx)
    zx_mu = self.z_mu_layers[idx](z_vec)
    zx_var = F.softmax(self.z_var_layers[idx](z_vec),-1)
    z_repar = self.gaussian_sample(zx_mu,zx_var)
    y_onehot = F.one_hot(torch.argmax(y_vec),self.n_states).unsqueeze(0).type(torch.DoubleTensor)
    return y_vec, y_prob, y_gumbel, z_vec, zx_mu, zx_var, z_repar

# test
x = y = all_train[6768]
infermodel = InferenceNet()
x_input = x.view((1,80))
y_vec, y_prob, y_gumbel, z_vec, zx_mu, zx_var, z_repar = infermodel(x_input)
print("y|x:",y_vec)
print("softmax(y|x):",y_prob)
print("q(y|x):",y_gumbel)
print("q(z|x,y):",z_vec)
print("q(z|x,y),mu+sigma*e",z_repar)

class GenerativeNet(nn.Module):
  def __init__(self,x_dim=80,n_states=5,z_parameter = 8,fc=True):
    super(GenerativeNet,self).__init__()
    self.x_dim = x_dim
    self.n_states = n_states
    self.z_parameter = z_parameter
    self.fcx = fc
    #1. p(z|y)
    self.zy_mu = nn.Linear(self.n_states,self.z_parameter)
    self.zy_var = nn.Linear(self.n_states,self.z_parameter)
    #2. p(x|z)
    self.FC_x = nn.Sequential(
        nn.Linear(self.z_parameter,int(self.x_dim/4)),
        nn.ReLU(),
        nn.Linear(int(self.x_dim/4),int(self.x_dim/2)),
        nn.ReLU()
    )
    self.NB_mu = nn.Linear(int(self.x_dim/2),int(self.x_dim))
    self.NB_theta = nn.Linear(int(self.x_dim/2),int(self.x_dim))
  def forward(self,z,y):
    #1. p(z|y)
    zy_mu1 = self.zy_mu(y)
    zy_var1 = F.softmax(self.zy_var(y),-1)
    #2. p(x|z)
    xz = self.FC_x(z)
    xz_mu = torch.exp(self.NB_mu(xz))
    xz_theta = torch.exp(self.NB_theta(xz))
    return zy_mu1,zy_var1,xz_mu,xz_theta

class GMVAE2(nn.Module):
  def __init__(self,x_dim=80,n_states=5,z_parameter = 8,gumbel_temperature=1.0,fc=True):
    super(GMVAE2,self).__init__()
    self.inference = InferenceNet(x_dim,n_states,z_parameter,gumbel_temperature)
    self.generator = GenerativeNet(x_dim,n_states,z_parameter,fc)
  def forward(self,x_input):
    y_vec, y_prob, y_gumbel, z_vec, zx_mu, zx_var, z_repar = self.inference(x_input) #here z|x,y also changed to y_vec from y_gumbel
    zy_mu1,zy_var1,xz_mu,xz_theta = self.generator(z_repar,y_gumbel) #if we change y_gumbel to y_vec #,xz_lambda
    return y_vec,z_repar,zx_mu,zx_var,zy_mu1,zy_var1,xz_mu,xz_theta,y_gumbel #xz_lambda,

class LossFunctions(nn.Module):
  def __init__(self):
    super(LossFunctions,self).__init__()

    self.eps = 1e-10
  def NB_loss(self,x_true,x_mu,x_theta): # reconstruction loss
    eps = self.eps
    theta = torch.clamp(x_theta,0,1e8)
    x_pred = x_mu
    t1 = torch.lgamma(theta+eps)+torch.lgamma(x_true+1.0)-torch.lgamma(x_true+theta+eps)
    t2 = (theta+x_true)*torch.log(1.0+(x_pred/(theta+eps))) + x_true*(torch.log(theta+eps)-torch.log(x_pred+eps))
    nbloss = (t1+t2).sum()
    return nbloss
  def ZINB(self,x_true,x_mu,x_theta,x_lambda):
    eps = self.eps
    theta = torch.clamp(x_theta,0,1e8)
    x_pred = x_mu
    pi = x_lambda
    nb_case = self.NB_loss(x_true,x_mu,x_theta) - torch.log(1.0-pi+eps)
    zero_nb = torch.pow(theta/(theta+x_pred+eps),theta)
    zero_case = -torch.log(pi+((1.0-pi)*zero_nb)+eps)
    result = torch.where((x_true>=1e-8),nb_case,zero_case)
    return result
  def KL_y(self,y_vec): # here we assume prior p(y) = 1/K, uniformly distributed among all categories
    # return -torch.mean(-torch.sum(F.softmax(y_vec,-1)*F.log_softmax(y_vec,-1))) 
    return torch.sum(F.softmax(y_vec,-1)*F.log_softmax(y_vec,-1))
  def KL_z(self,z,zx_mu,zx_var,zy_mu,zy_var):
    logzx = torch.sum(-0.5*torch.log(2.0*np.pi*(zx_var)+self.eps)-torch.pow(z-zx_mu,2)/(2*zx_var))
    logzy = torch.sum(-0.5*torch.log(2.0*np.pi*(zy_var)+self.eps)-torch.pow(z-zy_mu,2)/(2*zy_var))
    return logzx, logzy

"""## Training"""

model = GMVAE2(n_states=10,z_parameter = 5,fc=True)
loss_function = LossFunctions()
MSE_loss = nn.MSELoss()
optimizer = Adam(model.parameters(),lr=0.0001, weight_decay=0.00005)

batch_size = 1
num_epoch = 20
train_loss = []
for epoch in range(num_epoch):
  data_loader = DataLoader(all_train,batch_size=batch_size,shuffle=True)
  model.train()
  print("Epoch: " + str(epoch) + "/" + str(num_epoch - 1),", ",datetime.now())
  epoch_training_loss = []
  for i, batch in enumerate(data_loader):
    x0 = batch
    x_input = x0.view((batch_size,80))
    output = model(x_input)
    loss1 = loss_function.NB_loss(x_input,output[6],output[7]).sum()
    loss2 = loss_function.KL_y(output[8]) # should we go with y_vec or y_gumbel?
    loss3a, loss3b = loss_function.KL_z(output[1],output[2],output[3],output[4],output[5])
    loss = 10*loss1+max(0.05,-(loss2))+max(1,(loss3a-loss3b))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    epoch_training_loss.append(loss.item())
    if i % 200 == 0:
      print("loss1 = ",loss1.item(),", loss2 = ",-loss2.item(),", loss3a = ",loss3a.item(),", loss3b = ",-loss3b.item())
    if i % 1000 == 0:
      # print an example to check the decoder performance
      k = np.random.randint(0,len(all_train),1)[0]
      x_input = all_train[k].view((1,80))
      output = model(x_input)
      fig,axarr = plt.subplots(1,4,figsize=(20,5))
      axarr[0].imshow(all_train[k][:,0,:].detach().numpy())
      axarr[1].plot(range(x_input.shape[1]),x_input.detach().numpy()[0])
      axarr[1].scatter(range(x_input.shape[1]),x_input.detach().numpy()[0])
      axarr[1].plot(range(x_input.shape[1]),(output[6]).detach().numpy()[0])
      axarr[1].scatter(range(x_input.shape[1]),(output[6]).detach().numpy()[0])
      axarr[1].set_ylim(0,10)
      axarr[2].plot(range(output[0].shape[1]),output[8].detach().numpy()[0])
      axarr[2].scatter(range(output[0].shape[1]),output[8].detach().numpy()[0])
      axarr[2].plot(range(output[0].shape[1]),F.softmax(output[0],-1).detach().numpy()[0])
      axarr[2].scatter(range(output[0].shape[1]),F.softmax(output[0],-1).detach().numpy()[0])
      axarr[3].plot(range(len(output[1].detach().numpy()[0])),output[1].detach().numpy()[0])
      axarr[3].scatter(range(len(output[1].detach().numpy()[0])),output[1].detach().numpy()[0])
      plt.show()
  train_loss.append(epoch_training_loss)

"""# Results Visualization"""

z_repar_list = []
y_list = []
for k in range(len(all_train)):
  x_input = all_train[k].view((1,80))
  output = model(x_input)
  z_repar_list.append(output[1].detach().numpy()[0])
  y_list.append(output[0].detach().numpy())
x_list = []
for k in range(len(all_train)):
  x_input = all_train[k].view((1,80))
  x_list.append(x_input.detach().numpy()[0])

import umap
z_repar_array = np.array(z_repar_list)
z_repar_array.shape
reducer = umap.UMAP(n_components=3,n_neighbors=4,metric="euclidean")
embedding = reducer.fit_transform(z_repar_array)
embedding.shape
emdf = pd.DataFrame(embedding)
emdf.columns = ["umap1","umap2","umap3"]
emdf.loc[:,"group"] = np.argmax(np.array(y_list),2)
emdf.loc[:,"data_id"] = emdf.index.tolist()
import plotly.express as px
fig = px.scatter_3d(emdf, x='umap1', y='umap2', z='umap3',
                    color=[str(int(i[0])) for i in np.argmax(np.array(y_list),2).tolist()],
                    hover_name="data_id",
                    color_discrete_sequence=px.colors.qualitative.Set1)
fig.show()

k = np.random.randint(0,len(all_train),1)[0]
k = 6401
print(k) #6406, -1093, 9292
x_input = all_train[k].view((1,80))
output = model(x_input)
print(output[0].detach().numpy())

fig,axarr = plt.subplots(3,5,figsize=(50,30),constrained_layout=True)
for k in range(len(klist)):
  x_input = all_train[k].view((1,80))
  output = model(x_input)
  axarr[0,k].imshow(all_train[k][:,0,:].detach().numpy(),vmin=0,vmax=10)
  axarr[1,k].plot(range(x_input.shape[1]),x_input.detach().numpy()[0])
  axarr[1,k].scatter(range(x_input.shape[1]),x_input.detach().numpy()[0])
  axarr[1,k].plot(range(x_input.shape[1]),output[6].detach().numpy()[0])
  axarr[1,k].scatter(range(x_input.shape[1]),output[6].detach().numpy()[0])
  axarr[2,k].scatter(x_input.detach().numpy()[0],output[6].detach().numpy()[0])
plt.show()

fig,axarr = plt.subplots(1,2,figsize=(20,8),constrained_layout=True)
sns.scatterplot(x="umap1",y="umap2",data=emdf,hue="group",ax=axarr[0],palette="Set1")
sns.scatterplot(x="umap1",y="umap3",data=emdf,hue="group",ax=axarr[1],palette="Set1")
axarr[0].set_title("UMAP2 vs. UMAP1",fontsize=20)
axarr[1].set_title("UMAP3 vs. UMAP1",fontsize=20)
fig.suptitle("UMAP projection of latent variable z \n (colored by group identified from GMM)", fontsize=30)
plt.show() #10 states, 5 parameters in z
